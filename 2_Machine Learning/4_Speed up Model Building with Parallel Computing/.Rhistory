#Importing datasets library
library(datasets)
library(caret)
library(randomForest)
library(e1071)
#Importing dhfr dataset
data(dhfr)
#Checking to see any null values
sum(is.na(dhfr))
#seting seed for maintaining reproducibilty
set.seed(100)
#random split of data for machine learning
TrainingIndex <- createDataPartition(dhfr$Y,p=0.8,list=FALSE)
TrainingSet <- dhfr[TrainingIndex,] #Training set
TestingSet <- dhfr[-TrainingIndex,] #Testing set
start.time <- proc.time()
Model <- train(Y ~ .,
data = TrainingSet, # Build model using training set
method = "rf", # Learning algorithm
tuneGrid = data.frame(mtry = seq(5,15, by=5))
)
stop.time <- proc.time()
run.time <- stop.time - start.time
print(run.time)
library(doParallel)
cl <- makePSOCKcluster(3) #i7 7th Gen has 4 cores, so using 3 so that 1 is free to handle other tasks from the PC
registerDoParallel(cl)
start.time <- proc.time()
Model <- train(Y ~ .,
data = TrainingSet, # Build model using training set
method = "rf", # Learning algorithm
tuneGrid = data.frame(mtry = seq(5,15, by=5))
)
stop.time <- proc.time()
run.time <- stop.time - start.time
print(run.time)
stopCluster(cl)
57.71/25.49
seq(5,15, by=5)
##############################
#Apply Model for Prediction
Model.training <- predict(Model,TrainingSet)
#Model Performance by confusion matrix
ModelPerformance <- confusionMatrix(Model.TrainingSet$Y)
#Model Performance by confusion matrix
ModelPerformance <- confusionMatrix(Model, TrainingSet$Y)
#Model Performance by confusion matrix
ModelPerformance <- confusionMatrix(Model.training, TrainingSet$Y)
print(ModelPerformance)
View(ModelPerformance)
View(ModelPerformance)
View(TrainingSet)
View(TrainingSet)
#Feature Importance
Importance <- varImp((Model))
plot(Importance)
plot(Importance,col="red",top = 25)
