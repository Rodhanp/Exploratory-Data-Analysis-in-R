#Importing Libraries
library(datasets)
library(caret)
#Importing the dhfr Dataset
data(dhfr)
#Check to see if there are missing data
sum(is.na(dhfr))
#To achieve reproducible model, set the random seed number
set.seed(100)
#Performs stratifies random splits of dataset
TrainingIndex <- createDataPartition(dhfr$Y, p=0.8, list=FALSE)
TrainingSet <- dhfr[TrainingIndex,] #Makes Training subset(80%)
TestingSet <- dhfr[-TrainingIndex,] #Makes Testing subset (20%)
#########################
#SVM model(Support vector model)-polynomial kernal
#Build Training Model
Model <- train(Y ~., data = TrainingSet,
method = "svmPoly",
na.action = na.omit,
preProcess=c("scale","center"),
trcontrol= trainControl(method="none"),
tuneGrid=data.frame(degree=1,scale=1,C=1) )
# Build Cross Validation (CV) Model
Model.CV <- train(Y ~., data=TrainingSet,
method="svmPoly",
na.action=na.omit,
preProcess=c("center","scale"),
trControl=trainControl(method="cv",number=10),
tuneGrid=data.frame(degree=1,scale=1,C=1))
#Apply Model for prediction
Model.TrainingSet <- predict(Model, TrainingSet) #Predict Training Set using the model
Model.TestingSet <- predict(Model, TestingSet) #Predict Testing Set using the model
Model.CrossValidation <- predict(Model.CV, TrainingSet) # Predict Cross Validation of the Training Set
#Model Performance-Displays Confusion Matrix and other statistics
Model.trainingset.confusion <- confusionMatrix(Model.TrainingSet, TrainingSet$Y)
Model.testingset.confusion <- confusionMatrix(Model.TestingSet, TestingSet$Y )
#Importing Libraries
library(datasets)
library(caret)
#Importing the dhfr Dataset
data(dhfr)
#View(dhfr)
#Check to see if there are missing data
sum(is.na(dhfr))
#To achieve reproducible model, set the random seed number
set.seed(100)
#Performs stratifies random splits of dataset
TrainingIndex <- createDataPartition(dhfr$Y, p=0.8, list=FALSE)
TrainingSet <- dhfr[TrainingIndex,] #Makes Training subset(80%)
TestingSet <- dhfr[-TrainingIndex,] #Makes Testing subset (20%)
#########################
#SVM model(Support vector model)-polynomial kernal
#Build Training Model
Model <- train(Y ~., data = TrainingSet,
method = "svmPoly",
na.action = na.omit,
preProcess=c("scale","center"),
trcontrol= trainControl(method="none"),
tuneGrid=data.frame(degree=1,scale=1,C=1) )
#Save Model to RDS
saveRDS(Model, "Model.rds")
#Read Model from RDS
read.Model <- readRDS("Model.rds")
#Apply Model for prediction
Model.TrainingSet <- predict(Model, TrainingSet) #Predict Training Set using the model
Model.TestingSet <- predict(Model, TestingSet) #Predict Testing Set using the model
#Model Performance-Displays Confusion Matrix and other statistics
Model.trainingset.confusion <- confusionMatrix(Model.TrainingSet, TrainingSet$Y)
Model.testingset.confusion <- confusionMatrix(Model.TestingSet, TestingSet$Y )
print(Model.trainingset.confusion)
print(Model.testingset.confusion)
#Feature Importance
Importance <- varImp(Model)
plot(Importance)
plot(Importance,col="red")
plot(Importance,col="red", top = 25) # Visualizing top 25 values for better representation
#Importing Libraries
library(datasets)
library(caret)
#Importing the dhfr Dataset
data(dhfr)
#Check to see if there are missing data
sum(is.na(dhfr))
#To achieve reproducible model, set the random seed number
set.seed(100)
#Performs stratifies random splits of dataset
TrainingIndex <- createDataPartition(dhfr$Y, p=0.8, list=FALSE)
TrainingSet <- dhfr[TrainingIndex,] #Makes Training subset(80%)
TestingSet <- dhfr[-TrainingIndex,] #Makes Testing subset (20%)
#Apply Model for prediction
Model.TrainingSet <- predict(read.Model, TrainingSet) #Predict Training Set using the model
Model.TestingSet <- predict(read.Model, TestingSet) #Predict Testing Set using the model
#Read Model from RDS
read.Model <- readRDS("Model.rds")
#Apply Model for prediction
Model.TrainingSet <- predict(read.Model, TrainingSet) #Predict Training Set using the model
Model.TestingSet <- predict(read.Model, TestingSet) #Predict Testing Set using the model
#Model Performance-Displays Confusion Matrix and other statistics
Model.trainingset.confusion <- confusionMatrix(Model.TrainingSet, TrainingSet$Y)
Model.testingset.confusion <- confusionMatrix(Model.TestingSet, TestingSet$Y )
print(Model.trainingset.confusion)
print(Model.testingset.confusion)
#Feature Importance
Importance <- varImp(Model)
plot(Importance)
plot(Importance,col="red")
plot(Importance,col="red", top = 25) # Visualizing top 25 values for better representation
View(read.Model)
#########################
#SVM model(Support vector model)-polynomial kernal
#Build Training Model
Model <- train(Y ~., data = TrainingSet,
method = "svmPoly",
na.action = na.omit,
preProcess=c("scale","center"),
trcontrol= trainControl(method="none"),
tuneGrid=data.frame(degree=1,scale=1,C=1) )
#Save Model to RDS
saveRDS(Model, "Model.rds")
#Read Model from RDS
read.Model <- readRDS("Model.rds")
#Importing Libraries
library(datasets)
library(caret)
#Importing the dhfr Dataset
data(dhfr)
#Check to see if there are missing data
sum(is.na(dhfr))
#To achieve reproducible model, set the random seed number
set.seed(100)
#Performs stratifies random splits of dataset
TrainingIndex <- createDataPartition(dhfr$Y, p=0.8, list=FALSE)
TrainingSet <- dhfr[TrainingIndex,] #Makes Training subset(80%)
TestingSet <- dhfr[-TrainingIndex,] #Makes Testing subset (20%)
#Read Model from RDS
read.Model <- readRDS("Model.rds")
#Apply Model for prediction
Model.TrainingSet <- predict(read.Model, TrainingSet) #Predict Training Set using the model
Model.TestingSet <- predict(read.Model, TestingSet) #Predict Testing Set using the model
#Model Performance-Displays Confusion Matrix and other statistics
Model.trainingset.confusion <- confusionMatrix(Model.TrainingSet, TrainingSet$Y)
Model.testingset.confusion <- confusionMatrix(Model.TestingSet, TestingSet$Y )
print(Model.trainingset.confusion)
print(Model.testingset.confusion)
#Feature Importance
Importance <- varImp(Model)
plot(Importance)
plot(Importance,col="red")
plot(Importance,col="red", top = 25) # Visualizing top 25 values for better representation
#Feature Importance
Importance <- varImp(read.Model)
plot(Importance)
plot(Importance,col="red")
plot(Importance,col="red", top = 25) # Visualizing top 25 values for better representation
#loading library
library(caret)
#loading dhfr dataset
data(dhfr)
#check for missing values
Sum(is.na(dhfr))
#check for missing values
sum(is.na(dhfr))
#function to add missing values in the dataset
na.gen<-function(data,n){
i <- 1
while (i < n+1) {
idx1 <- sample(1:nrow(data), 1)
idx2 <- sample(1:ncol(data), 1)
data[idx1,idx2] <- NA
i = i+1
}
return(data)
}
#Remove the Y column before introducing the NA values
dhfr <- dhfr[,-1]
View(dhfr)
#adding NA to dataset
dhfr <- na.gen(dhfr,100)
#Check for missing data
sum(is.na(dhfr))
colSums(is.na(dhfr))
str(dhfr)
view(colSums(is.na(dhfr)))
View(colSums(is.na(dhfr)))
#List rows with missing data
missingdata <- dhfr[!com[lete.cases(dhfr),]]
#List rows with missing data
missingdata <- dhfr[!complete.cases(dhfr),]
View(missingdata)
sum(is.na(missingdata))
# If above sum is 0, this means that there is no missing data and proceed to modeling.
# If above sum is greater than 0, then proceed to handling the missing data.
# There are 2 options-
# 1. Simply delete all entries with missing data
clean.Data <- na.omit(dhfr)
sum(is.na(clean.Data))
dhfr.impute <- dhfr
for (i in which(sapply(dhfr.impute, is.numeric))) {
dhfr.impute[is.na(dhfr.impute[, i]), i] <- mean(dhfr.impute[, i],  na.rm = TRUE)
}
sum(is.na(dhfr.impute))
# Median
dhfr.impute <- dhfr
for (i in which(sapply(dhfr.impute, is.numeric))) {
dhfr.impute[is.na(dhfr.impute[, i]), i] <- median(dhfr.impute[, i],  na.rm = TRUE)
}
sum(is.na(dhfr.impute))
